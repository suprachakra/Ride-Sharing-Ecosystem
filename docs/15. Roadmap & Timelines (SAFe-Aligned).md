[15. Roadmap & Timelines (SAFe-Aligned)](#15-roadmap--timelines-safe-aligned)

---

### 15. Roadmap & Timelines (SAFe-Aligned) 

**Roadmap Goals:**
Align features, data collection, and pilots with incremental delivery and validation. Each PI delivers testable increments. If pilots fail KPIs, refine and retest next PI rather than prematurely scaling.


| **Timeframe & Iterations**                                                                                 | **Phase & Objectives**                                                               | **Key Activities / Tasks**                                                                                                                                                                                                 | **Integrated Epics**                                                                                          | **Teams Involved**                                                                                | **Dependencies**                                                                                                                                      | **Iteration Goals & Success Metrics**                                                                                                                                                                                                   | **Deliverables**                                                                                                                                                 | **SAFe Milestones**                                                                                                                       | **Risks & Mitigation**                                                                                                                                                                                                                           | **Outcomes**                                                                                                                                                             |
|-------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **Q1 / Weeks 1–2 & 3–4 (Sprints 1 & 2)**                                                                    | **Phase 0: Pre-Planning & Feasibility**<br>*(~4 weeks total)*                         | - Conduct **regulatory feasibility** (driver union constraints, local regs).<br>- Finalize **budget ceilings** & resources with Finance.<br>- Define **minimal param-based surge & seat-based discount** scope for MVP.<br>- Establish **Steering Committee**, finalize **RACI matrix**.<br>- Lock a realistic **MVP timeline** in line with resource constraints.   | - Early scoping for **Dynamic Pricing** & **Compliance**.                                                          | - Product (Strategy)<br>- Finance (Budget)<br>- Compliance (Local regs)<br>- Ops (Driver union input)<br>- Engineering (Feasibility)   | - Legal constraints in each operational zone.<br>- Budget approvals from Finance.                                                                                     | - **Goal**: Confirm MVP viability & get departmental buy-in.<br>- **Success**: Feasibility plan validated by all teams; no major showstoppers.<br>- **Metrics**: 100% RACI clarity; Budget validated within ±10% margin. | 1. **Feasibility Report**<br>2. **High-level MVP Definition** (param-based surge + seat-based discount)<br>3. **RACI Matrix** & Steering Committee charter | - **PI Planning** alignment (Week 4).<br>- **Iteration Planning** (weekly).                                             | - **Risk**: Overly optimistic budget <br>**Mitigation**: Add buffer ~10% to cost estimates; confirm fallback scope if budget cut.<br><br>- **Risk**: Unclear responsibilities <br>**Mitigation**: Formal sign-off of RACI matrix by Steering Committee. | - Clear, realistic MVP definition<br>- Formal governance & role clarity<br>- Solid feasibility for immediate next steps                                                                 |
| **Q1 / Weeks 5–6 & 7–8 (Sprints 3 & 4)**                                                                    | **Phase 1: MVP Requirements & Architecture**<br>*(~4 weeks total)*                   | - Detail **MVP functional requirements** for param-based surge, seat discount, and minimal “Why Fare?” UI.<br>- Define **baseline architecture**: microservices, containerization approach.<br>- Set up **dev & test environments**; implement basic **logging & monitoring**.<br>- Draft **compliance toggles** for surge caps. | - **Dynamic Pricing** (Param-based) <br>- **Transparency & UX** (Basic “Why Fare?”) <br>- **Compliance** (Surge toggles)              | - Product (Requirement grooming)<br>- Engineering (Env setup, Arch)<br>- QA (Test harness design)<br>- Ops (Driver readiness input)<br>- Compliance | - Signed-off MVP scope from Phase 0.<br>- Final budget clarity for initial cloud infra.                                                                   | - **Goal**: Lock requirements & stable baseline architecture.<br>- **Success**: MVP Feature Freeze; dev environment stable at 50% load in test.<br>- **Metrics**: 80% readiness on environment; Logging coverage for all MVP flows.        | 1. **MVP Requirements Doc**<br>2. **Baseline Architecture Diagrams**<br>3. **Compliance Surge Toggles**                                                            | - **ART Sync** every 2 weeks.<br>- **System Demo** (Week 10) to show baseline environment & toggles.                                                   | - **Risk**: Feature creep <br>**Mitigation**: Product to enforce backlog gating & freeze after sign-off.<br><br>- **Risk**: Under-resourced environment <br>**Mitigation**: Start minimal; scale container cluster with usage metrics.                   | - Requirements fully locked<br>- Architecture & environment validated<br>- Clear path to develop MVP features                                                                                      |
| **Q1,Q2 / Weeks 9–10,11–12 + **Q4 (Sprints 1–2)**(Sprints 5,6,7)**                                          | **Phase 2: MVP Development & System Integration Testing (SIT)**<br>*(~4 weeks total)* | - Develop **core surge logic**, seat discount algorithms, and “Why Fare?” UI.<br>- **SIT** environment prep: integrate microservices, run initial SIT with test data.<br>- Implement **logging** & alerting for concurrency; test basic failover.                                     | - **Dynamic Pricing** <br>- **Transparency & UX**                                                                 | - Engineering (Core dev)<br>- QA (SIT, regression)<br>- Ops (Training outline in parallel)<br>- Compliance (Review toggles)         | - Completed baseline architecture from Phase 1.<br>- SIT environment readiness.                                                                                           | - **Goal**: Build stable MVP in SIT environment.<br>- **Success**: <1% critical defects in SIT; sub-2s response time for 80% requests.<br>- **Metrics**: 75% SIT coverage on core flows; validated concurrency toggles.                               | 1. **MVP SIT Build**<br>2. **SIT Test Results**<br>3. **Updated Surge & Discount Modules**                                                               | - **Iteration Planning** (Weekly).<br>- **ART Sync** (Biweekly).                                                                                        | - **Risk**: Performance slowdowns <br>**Mitigation**: Early load testing & caching strategy.<br><br>- **Risk**: Toggles break concurrency tests <br>**Mitigation**: Patch or revert toggles for launch if failing SIT.                            | - MVP components integrated in SIT<br>- Confidence in concurrency & performance<br>- Ready for UAT and pilot prep                                                                                                |
| **Q2 / Weeks 3–4, 5–6, 7-8 (Sprints 7,8,9)**                                                                | **Phase 3: UAT & Final Pilot Preparations**<br>*(~4 weeks total)*                     | - **User Acceptance Testing (UAT)** with selected internal + pilot driver reps.<br>- Validate UI clarity, gather user feedback on seat discount, “Why Fare?” messages.<br>- Freeze code, prep **pilot deployment** plan.<br>- Draft **runbooks** for pilot go-live.                  | - **Transparency & UX** <br>- **Driver Incentive** (seat discount)                                                | - QA (UAT management)<br>- Product (UAT acceptance criteria)<br>- Ops (driver training materials)<br>- DevOps (runbook prep)         | - SIT sign-off from Phase 2.<br>- Key stakeholder availability for UAT feedback.                                                                                           | - **Goal**: Ensure real users accept and sign off MVP readiness.<br>- **Success**: 90% acceptance from pilot testers; minimal user confusion calls (<5%).<br>- **Metrics**: 100% test script coverage in UAT; seat discount usage≥70% among testers. | 1. **UAT Feedback Report**<br>2. **Pilot Deployment Plan & Runbook**<br>3. **Release Go/No-Go Checklist**                                                          | - **System Demo** (Week 10 or 16, depending on final UAT timing).<br>- **Inspect & Adapt** (Week 12).                                               | - **Risk**: Last-minute critical defects <br>**Mitigation**: Strict freeze 2 weeks prior to pilot; immediate patch if severity high.<br><br>- **Risk**: Pilot confusion <br>**Mitigation**: Thorough training docs & driver Q&A sessions.                 | - UAT sign-off and readiness for pilot<br>- Clear runbooks for pilot deployment<br>- High confidence from pilot testers                                                                                   |
| **Q2 / Weeks 9–10 & 11–12 (Sprints 10 & 11)**                                                               | **Phase 4: MVP Pilot Launch**<br>*(~4 weeks total, **1 Quarter Monitoring** begins after Launch)* | - Launch **pilot** in a controlled district/zone (5–10% of overall user base).<br>- Train drivers with seat discount & surge explanation sessions.<br>- **24/7 monitoring** to track errors, confusion calls, acceptance rates.<br>- Begin official **1-quarter monitoring** clock once pilot is stable (approx end of Sprint 10).              | - **Dynamic Pricing** (Pilot rollout)<br>- **Driver Incentive**                                                | - Ops (Driver training, field support)<br>- Product (pilot configurations)<br>- DevOps (Monitoring dashboards)<br>- SRE (On-call) | - UAT sign-off from Phase 3.<br>- Budget allocated for pilot marketing or incentives.                                                                                      | - **Goal**: Validate real-world usage in pilot region.<br>- **Success**: On-time rides≥85%, confusion calls<5%, driver acceptance≥60%.<br>- **Metrics**: Pilot stability≥2 weeks with no major incidents; average surge response time<2s.        | 1. **Pilot Deployment**<br>2. **Pilot Monitoring Dashboards**<br>3. **Weekly Pilot Review Reports**                                                            | - **ART Sync** (Biweekly).<br>- **Quarterly Review** (Week 12 from pilot start).                                                                        | - **Risk**: Low pilot adoption <br>**Mitigation**: Targeted marketing & driver incentive top-ups.<br><br>- **Risk**: Unforeseen production issues <br>**Mitigation**: Canary release approach for rolling out pilot over 2–3 days.                        | - MVP tested in real environment<br>- 1 Quarter Monitoring initiated to refine usage, gather feedback<br>- Foundation for broader rollout                                                                        |
| **Q3 / Weeks 1–2, 3-4,5  (Sprints 13,14,15,16,17))**                                                        | **Phase 5: Pilot Monitoring & Stabilization**<br>*(~6 weeks to complete 1 Quarter post-pilot)* | - Continue **pilot** for a full quarter (~12 weeks from launch).<br>- Gather user & driver feedback, fix urgent defects or usability gaps.<br>- Evaluate compliance toggles under real load; refine logs & alerts.<br>- Conduct **retrospectives** mid-quarter to adjust seat discount parameters if needed. | - **Compliance** (Real usage toggles)<br>- **Transparency & UX** (Refinements)                                                   | - Ops (Ongoing pilot ops)<br>- SRE (Stability & performance)<br>- QA (Ongoing regression for hotfixes)<br>- Product (Feature refinement) | - Steady pilot environment from Phase 4.<br>- Steering Committee approvals for any major changes.                                                                         | - **Goal**: Achieve stable pilot over an entire quarter.<br>- **Success**: Driver acceptance≥70%; consistent <2% error rate; positive user feedback on seat discount.<br>- **Metrics**: MTTR<1 hour for any major defect; user satisfaction≥80% in pilot region.          | 1. **Pilot Quarter Report**<br>2. **Refined Seat Discount Logic**<br>3. **Compliance Toggle Adjustments**                                                      | - **System Demo** (at end of quarter, e.g. Week 16 or 22).<br>- **Inspect & Adapt** (Week 24).                                                        | - **Risk**: Pilot dissatisfaction if discount is unclear <br>**Mitigation**: Ongoing driver comms, UI tweaks via OTA updates.<br><br>- **Risk**: Pilot data skewed by small region <br>**Mitigation**: Expand pilot boundaries carefully if data is insufficient.           | - Stable pilot validated after 1 quarter<br>- Data-driven improvements integrated<br>- Confidence to plan citywide expansion                                                                               |
| **Q3 / Weeks 6, 7-8, 9 (Sprints 18 & 19)**                                                                  | **Phase 6: Measured Citywide Expansion**<br>*(~4 weeks to initiate expansion)*        | - Expand coverage from pilot zone to ~25–50% of city.<br>- Continue **driver training** at scale, ramp up **marketing** (referral codes, discounts).<br>- Maintain **param-based surge** (no ML yet), ensuring performance at scale.<br>- **Monitor** expansion for first 2 sprints to confirm stability. | - **Dynamic Pricing** <br>- **Marketing & GTM**                                                                 | - Ops (Citywide training)<br>- Marketing (Citywide campaign)<br>- DevOps (Scaling microservices)<br>- Finance (Budget for expansion) | - Positive pilot results from Phase 5.<br>- Additional budget for marketing & driver outreach.                                                                     | - **Goal**: Successfully scale to half the city.<br>- **Success**: On-time≥90%, driver acceptance≥75%, no major performance degrade at scale.<br>- **Metrics**: <2s response time under 50% city load, confusion calls remain<5%.                          | 1. **Expansion Plan Doc**<br>2. **Updated Training Materials**<br>3. **Expanded Monitoring Dashboards**                                                           | - **ART Sync** (Biweekly).<br>- **Solution Train Sync** if multiple ARTs involved (Weeks ~19, 23).                                                       | - **Risk**: Infrastructure overload <br>**Mitigation**: Pre-increase container capacity + load balancer config.<br><br>- **Risk**: Marketing overspend <br>**Mitigation**: Weekly burn-rate reviews, pivot marketing approach if ROI < target.                 | - Seamless expansion to partial city coverage<br>- Larger user base feedback for further refining<br>- No ML complexity introduced yet, ensuring reliability                                                                 | 
| **Q3 / Weeks 10, 11–12 (Sprints 20,21)** + **Q4 (Sprints 22–27)**                                           | **Phase 7: Full City Rollout & 3-Quarter Observations**<br>*(~9 months from pilot end)* | - After stable partial expansion, progress to **full city coverage** gradually.<br>- Over the next **3 quarters** (including the remainder of Q3 + full Q4 + Q1 next year), systematically observe operational metrics, refine param-based surge pricing, and ensure compliance. <br>- Continue to gather feedback from drivers & users. <br>- **No advanced ML** is introduced; focus on ensuring reliability, compliance, user satisfaction. | - **Scalability** <br>- **Compliance & Risk Mgmt**                                                                       | - Ops (All city ops, driver events)<br>- QA & SRE (Long-term stability checks)<br>- Finance (Funding for any scale-ups)<br>- Product (Ongoing improvements) | - Must have stable partial coverage (Phase 6).<br>- City regulators’ approvals for surge caps citywide.                                                                 | - **Goal**: Full city adoption without ML overhead.<br>- **Success**: NPS≥4.5 or equivalent rating; churn<5%; compliance checks pass monthly audits.<br>- **Metrics**: System uptime≥99.9%; driver acceptance≥80% citywide; cost overhead <5% beyond plan.           | 1. **Full Citywide Rollout**<br>2. **Quarterly Progress Reports**<br>3. **Refined Pricing Parameter Docs**                                                       | - **Quarterly Review** at the end of each quarter (Weeks ~24, 36, 48, etc.).<br>- **Inspect & Adapt** after major expansions.                                      | - **Risk**: Feature stagnation without ML <br>**Mitigation**: Keep param-based surge refined with real data; plan ML only after 3 quarters of stable operations.<br><br>- **Risk**: Compliance drift in certain zones <br>**Mitigation**: Periodic legal checks. | - Entire city covered successfully<br>- Thorough data-driven improvements over 3 quarters<br>- Foundation for an eventual ML-based approach without sacrificing stability                                                                |
| **After ~3 Full Quarters of Observations Post-Citywide Rollout**                                           | **(Future) Phase 8: ML Feasibility & Gradual Introduction**                          | - Evaluate advanced ML surge logic only if param-based approach is stable for **≥3 quarters** citywide.<br>- Conduct feasibility on ML model costs, potential gains (>2–3% improvement).<br>- Plan a **small ML pilot** in parallel, using shadow mode first.                                | - **Advanced Dynamic Pricing**                                                                                     | - Data/Analytics (ML modeling)<br>- Engineering (ML pipeline, feature flags)<br>- Product (Scope and ROI)                            | - Must have robust historical data (≥3 quarters) to train ML.<br>- Budget sign-off for additional infra.                                                           | - **Goal**: Validate if ML truly benefits beyond param-based surge.<br>- **Success**: ML tests demonstrate ROI≥X% or better user experience with no compliance breach.<br>- **Metrics**: If overhead >2%, revert or freeze advanced ML.                                  | 1. **ML Feasibility Report**<br>2. **Shadow Mode ML Test Results**<br>3. **Go/No-Go Decision for ML Rollout**                                                      | - **PI Planning** in future increments.<br>- Potential new **System Demo** if ML is introduced.                                                              | - **Risk**: Over-engineered ML with minimal ROI <br>**Mitigation**: Strict ROI targets & shadow mode testing before citywide ML.<br><br>- **Risk**: Regulatory unpredictability <br>**Mitigation**: Real-time compliance toggles remain for advanced ML.                   | - Potential next-generation surge approach if ML is worthwhile<br>- Protect operational stability while exploring new tech                                                                                 |

---

### **Key Notes & Assumptions**

1. **2-Week Iterations**: Every sprint is 2 weeks, with **Iteration Planning** occurring weekly or at the start of each sprint. **ART Sync** happens every 2 weeks to align teams across the Agile Release Train.
2. **SAFe Milestones**:
   - **PI Planning**: Occurs approximately at Weeks 4, 10, 18, etc., to plan increments and backlog refinement.  
   - **System Demo**: At key increments (e.g., Weeks 10, 16, 22) or after major deliverables like SIT completion or UAT.  
   - **Inspect & Adapt**: Retrospectives and improvements at mid- and end-of-quarter (Weeks 12, 24).  
   - **Solution Train Sync**: If multiple ARTs/solution trains exist, coordinate around Weeks 19, 23.  
   - **Quarterly Review**: High-level review of progress, potential re-budgeting, next PI objectives (Week 24 from pilot start or per your org’s cadence).
3. **Release Management**:  
   - **Pre-Release**: Phases 0–2 revolve around locking requirements, SIT, and preparing for pilot.  
   - **Canary/Pilot**: Phase 4 uses a small-target canary approach (5–10% traffic) ramping up to 100% in the pilot zone.  
   - **Post-Release**: Phase 5 includes at least one quarter of real-world observation. Subsequent expansions are also incremental, each with canary-like checks.  
   - **Rollback Plan**: Always maintain versioned releases with automated rollback scripts if error rates exceed thresholds.  
4. **Monitoring & Stabilization**:  
   - Once **MVP pilot** launches, we keep it live for a **full quarter** (Phase 5).  
   - After stable pilot outcomes, we expand citywide and observe for **3 quarters** before **ML** is even considered.  
5. **Realistic Buffers**: Each phase has built-in buffers for risk management, budget, compliance issues, or pilot feedback.  
